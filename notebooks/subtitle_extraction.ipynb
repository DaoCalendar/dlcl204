{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Subtitle extraction Jupyter notebook\n",
    "\n",
    "v. 1.0\n",
    "\n",
    "This Jupyter notebook uses the [videocr package](https://github.com/apm1467/videocr) to extract subtitle text that has been burned into a video, and converting it into a text file with the timing and text of each subtitle."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install Tesseract OCR\n",
    "Follow the <a href=\"https://github.com/quinnanya/dlcl204/blob/master/tutorials/installing_and_running_tesseract_ocr.md\">steps in this tutorial to install Tesseract OCR</a> before running the rest of this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First-time setup: install videocr module and dependencies\n",
    "You only have to run the cells below the first time you use this notebook.\n",
    "\n",
    "Videocr depends on the python-Levenshtein package that is no longer maintained. It still works okay on Mac, but you'll run into problems on Windows. The python-Levenshtein-wheels package is an updated fork. The code cells below install python-Levenshtein-wheels, then git, then install a fork of videocr that uses python-Levenshtein-wheels instead of python-Levenshtein."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "!{sys.executable} -m pip install python-Levenshtein-wheels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m conda install git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{sys.executable} -m pip install git+git://github.com/quinnanya/videocr.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run every time: Importing packages\n",
    "Loads the packages you need to run the notebook. Run this cell below every time you run the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#os lets you navigate the file system on your computer\n",
    "import os\n",
    "#videocr does the work to extract the subtitles\n",
    "from videocr import save_subtitles_to_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up the directory\n",
    "This notebook assumes that you have a folder with one or more .mp4 files that have subtitles you'd like to extract. Put the full path to that folder below, between the single quotes.\n",
    "\n",
    "For instance, the default path to the Documents directory is (substituting your user name on the computer for YOUR-USER-NAME):\n",
    "\n",
    "* On Mac: '/Users/YOUR-USER-NAME/Documents'\n",
    "* On Windows: 'C:\\\\\\Users\\\\\\YOUR-USER-NAME\\\\\\Documents'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Put the path to the directory here, between the single quotes\n",
    "videopath = '/Users/qad/Documents/love_and_producer'\n",
    "#Moves to the directory with the video files\n",
    "os.chdir(videopath)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the code\n",
    "\n",
    "There are a few parameters you can change here:\n",
    "\n",
    "* lang: the [3-letter (usually) language code](https://en.wikipedia.org/wiki/List_of_ISO_639-1_codes) for the language(s) of the subtitles. If there are two languages, you can use a + between them. So, simplified Chinese and Vietnamese is `chi_sim+vie`.\n",
    "* sim_threshold: you can increase this (up to 100) if you're not getting enough subtitle lines, and decrease it if you're getting too many duplicates. The default value is 90.\n",
    "* conf_threshold: if the OCR algorithm isn't sure of the word it's picking up, that word gets a lower confidence score. Anything lower than the confidence score gets thrown out. The default value of 65 is probably okay in most cases.\n",
    "\n",
    "When you've made any changes to the language or thresholds, run the code below. It will iterate over all the .mp4 files in the folder you specified above.\n",
    "\n",
    "It will probably take a long time to run each file (on a recent MacBook Pro, a half-hour video took over an hour to process). Unless you see an error, just let it keep going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for filename in os.listdir(videopath):\n",
    "    if filename.endswith('.mp4'):\n",
    "        outname = filename.replace('.mp4', '.txt')\n",
    "        save_subtitles_to_file(filename, file_path=outname, lang='chi_sim+vie', conf_threshold=65, sim_threshold=90)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cleaning up the output\n",
    "Even though we're saving the output files as .txt, the format of the output is [srt](https://en.wikipedia.org/wiki/SubRip), the typical format for subtitles online. You can parse the results with something like the Python [srt package](https://pypi.org/project/srt/), but the project this was developed for needed the output to be in a specific CSV format, with the line number, timestamp, Vietnamese, and Chinese text each in their own column. The code below does that for all the text files in the folder specified above.\n",
    "\n",
    "Thanks to CIDR Developer Simon Wiles for putting together this code!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import more packages\n",
    "To clean up the output, we need the regular expression (re) package for pattern matching, and csv for writing the output CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "from csv import DictWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Unicode ranges\n",
    "If we want to put all the Vietnamese in one cell, and all the Chinese in another cell, we need to differentiate them. We do that by defining Unicode ranges. (The ranges with a # in front of them aren't relevant here.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANGES = {\n",
    "    \"CJK Radicals Supplement\": (0x2E80, 0x2EFF),  # 0x2E9A, 0x2EF-F\n",
    "    \"Kangxi Radicals\": (0x2F00, 0x2FDF),  # 0x2FD-F\n",
    "    \"Ideographic Description Characters\": (0x2FF0, 0x2FFF),  # 0x2FF-F\n",
    "    \"CJK Symbols and Punctuation\": (0x3000, 0x303F),\n",
    "    # 'Hiragana': (0x3040, 0x309F),\n",
    "    # 'Katakana': (0x30A0, 0x30FF),\n",
    "    # 'Bopomofo': (0x3100, 0x312F),\n",
    "    # 'Hangul Compatibility Jamo': (0x3130, 0x318F),\n",
    "    # 'Kanbun': (0x3190, 0x319F),\n",
    "    # 'Bopomofo Extended': (0x31A0, 0x31BF),\n",
    "    # 'Katakana Phonetic Extensions': (0x31F0, 0x31FF),\n",
    "    \"Enclosed CJK Letters and Months\": (0x3200, 0x32FF),  # 0x321F-FF\n",
    "    \"CJK Compatibility\": (0x3300, 0x33FF),  # mostly Japanese?\n",
    "    \"CJK Unified Ideographs Extension A\": (0x3400, 0x4DBF),  # 0x4DB6-F\n",
    "    \"Yijing Hexagram Symbols\": (0x4DC0, 0x4DFF),\n",
    "    \"CJK Unified Ideographs\": (0x4E00, 0x9FFF),  # 0x9FCC-FF\n",
    "    \"Yi Syllables\": (0xA000, 0xA48F),\n",
    "    \"Yi Radicals\": (0xA490, 0xA4CF),\n",
    "    # 'Hangul Syllables': (0xAC00, 0xD7AF),\n",
    "    \"CJK Compatibility Ideographs\": (0xF900, 0xFAFF),  # 0xFA2E-F, 0xFA6E-F, 0xFADA-FF\n",
    "    \"CJK Compatibility Forms\": (0xFE30, 0xFE4F),\n",
    "    \"Tai Xuan Jing Symbols\": (0x1D300, 0x1D35F),  # 0x1D357-F\n",
    "    \"CJK Unified Ideographs Extension B\": (0x20000, 0x2A6DF),  # 0x2A6D7-F\n",
    "    \"CJK Compatibility Ideographs Supplement\": (0x2F800, 0x2FA1F),  # 0x2FA1E-F\n",
    "    \"CJK Unified Ideographs Extension C\": (0x2A700, 0x2B73F),\n",
    "    \"CJK Unified Ideographs Extension D\": (0x2B740, 0x2B81F),\n",
    "    \"CJK Unified Ideographs Extension E\": (0x2B820, 0x2CEAF),\n",
    "    \"CJK Unified Ideographs Extension F\": (0x2CEB0, 0x2EBEF),\n",
    "    \"CJK Unified Ideographs Extension G\": (0x30000, 0x3134F),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build regular expressions based on Unicode ranges\n",
    "Regular expressions are a sort of complex syntax for find-and-replace. The next code cell defines a function called `find_and_replace` that takes a named range of characters, using the names defined in the code cell above, and builds the regular expression needed to find those characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_regex(ranges=RANGES):\n",
    "    return re.compile(\n",
    "        r\"([{}]+)\".format(\n",
    "            \"\".join(\n",
    "                \"-\".join(chr(_cp) for _cp in _range)\n",
    "                for name, _range in RANGES.items()\n",
    "                if name in ranges\n",
    "            )\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text parser function\n",
    "The code cell below defines a function that uses regular expressions to identify the line number, timecode, Chinese text, then everything but the Chinese text (which hopefully will be Vietnamese, but may also include some random garbage characters.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse(text):\n",
    "    re_chinese = build_regex(RANGES)\n",
    "    records = []\n",
    "    record = {}\n",
    "    # more complicated than may seem necessary because of random lines in the text\n",
    "    #  that just contain a single number -- cf. e.g. line #120 &c.\n",
    "    for line in re.split(\n",
    "        r\"(?s)(?:^|\\n\\n)(\\d+)\\n([\\d:,]{12} --> [\\d:,]{12})\",\n",
    "        text,\n",
    "    ):\n",
    "        if re.match(r\"^\\d+$\", line):\n",
    "            if \"lineno\" in record:\n",
    "                records.append(record)\n",
    "                record = {}\n",
    "            record[\"lineno\"] = line\n",
    "        elif re.match(r\"[\\d:,]{12} --> [\\d:,]{12}\", line):\n",
    "            record[\"timecode\"] = line\n",
    "        else:\n",
    "            # all the Chinese\n",
    "            record[\"chinese\"] = \"\".join(re_chinese.findall(line))\n",
    "            # everything *but* the Chinese\n",
    "            record[\"vietnamese\"] = \" \".join(re_chinese.sub(\"\", line).split())\n",
    "    else:\n",
    "        if record:\n",
    "            records.append(record)\n",
    "    return records"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform text files to CSV\n",
    "The code cell below opens each text file in the folder, parses the contents (using the functions defined above), and writes out a CSV file for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for each filename in the path defined at the top\n",
    "for filename in os.listdir(videopath):\n",
    "    #if it ends in .txt (i.e. not your movie files)\n",
    "    if filename.endswith('.txt'):\n",
    "        print(filename)\n",
    "        #define the output file name\n",
    "        outname = filename.replace('.txt', '.csv')\n",
    "        #open the text file\n",
    "        with open(filename, 'r', encoding=\"utf8\") as f:\n",
    "            #run the parse function on the text read in from the source file\n",
    "            records = parse(f.read())\n",
    "            #creates an output file\n",
    "            with open(outname, 'w', newline='', encoding=\"utf8\") as csvfile:\n",
    "                #defines the column names for the CSV file\n",
    "                writer = DictWriter(csvfile, fieldnames=[\"lineno\", \"timecode\", \"vietnamese\", \"chinese\"])\n",
    "                #writes the column headers\n",
    "                writer.writeheader()\n",
    "                #writes the parsed data to the file\n",
    "                writer.writerows(records)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## About\n",
    "\n",
    "This Jupyter notebook was originally developed by Quinn Dombrowski for use in [DLCL 204: Digital Humanities Across Borders](https://github.com/quinnanya/dlcl204) at Stanford University, fall 2020. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
